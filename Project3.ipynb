{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 3\n",
    "### Amanda Arce, Monu Chacko, Abdelmalek Hajjam, Nick Schettini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using any of the three classifiers described in chapter 6 of Natural Language Processing with Python, and any features you can think of, build the best name gender classifier you can.\n",
    "Begin by splitting the Names Corpus into three subsets: 500 words for the test set, 500 words for the dev- test set, and the remaining 6900 words for the training set. Then, starting with the example name gender classifier, make incremental improvements. Use the dev-test set to check your progress. Once you are satisfied with your classifier, check its final performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import names\n",
    "import random\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from string import ascii_lowercase\n",
    "\n",
    "\n",
    "#nltk.download('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])\n",
    "#shuffle the names\n",
    "random.shuffle(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let divide the data into test, dev and training datasets with 500, 500, x data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(names))\n",
    "#unpacking the names to 3 sets\n",
    "test, dev_test, training = names[:500], names[500:1000], names[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The gender feature 1 extractor uses first letter, last letter and suffix as its feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features1(name):\n",
    "    features = {}\n",
    "    features[\"firstletter\"] = name[0].lower()\n",
    "    features[\"lastletter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
    "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
    "    features[\"suffix2\"] = name[-2:].lower()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data using Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the dev using Feature 1 is: 0.792\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features1(n), g) for (n,g) in training]\n",
    "dev_test_set = [(gender_features1(n), g) for (n,g) in dev_test]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "acc_dev_test_1 = nltk.classify.accuracy(classifier, dev_test_set)\n",
    "print(\"The accuracy for the dev using Feature 1 is: \" + str(acc_dev_test_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the test using Feature 1 is: 0.804\n"
     ]
    }
   ],
   "source": [
    "# Performance test - Feature 1\n",
    "test_set = [(gender_features1(n), g) for (n,g) in test]\n",
    "test_set_1 = nltk.classify.accuracy(classifier, test_set)\n",
    "print(\"The accuracy for the test using Feature 1 is: \" + str(test_set_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The gender feature 2 extractor uses first letter, last letter and two suffixes as its feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features2(name):\n",
    "    features = {}\n",
    "    features[\"firstletter\"] = name[0].lower()\n",
    "    features[\"lastletter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
    "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
    "    features[\"suffix2\"] = name[-2:].lower()\n",
    "    features[\"suffix3\"] = name[-3:].lower()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train feature 2 using Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the dev using Feature 2 is: 0.81\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features2(n), g) for (n,g) in training]\n",
    "dev_test_set = [(gender_features2(n), g) for (n,g) in dev_test]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "acc_dev_test_2 = nltk.classify.accuracy(classifier, dev_test_set)\n",
    "print(\"The accuracy for the dev using Feature 2 is: \" + str(acc_dev_test_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the test using Feature 2 is: 0.822\n"
     ]
    }
   ],
   "source": [
    "# Performance test - Feature 2\n",
    "test_set = [(gender_features2(n), g) for (n,g) in test]\n",
    "test_set_2 = nltk.classify.accuracy(classifier, test_set)\n",
    "print(\"The accuracy for the test using Feature 2 is: \" + str(test_set_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The gender feature 3 extractor uses first letter, last letter and three suffixes as its feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features3(name):\n",
    "    features = {}\n",
    "    features[\"firstletter\"] = name[0].lower()\n",
    "    features[\"lastletter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
    "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
    "    features[\"suffix2\"] = name[-2:].lower()\n",
    "    features[\"suffix3\"] = name[-3:].lower()\n",
    "    features[\"prefix3\"] = name[:3].lower()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train feature 3 data using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the dev using Feature 3 is: 0.826\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features3(n), g) for (n,g) in training]\n",
    "dev_test_set = [(gender_features3(n), g) for (n,g) in dev_test]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "acc_dev_test_3 = nltk.classify.accuracy(classifier, dev_test_set)\n",
    "print(\"The accuracy for the dev using Feature 3 is: \" + str(acc_dev_test_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the test using Feature 3 is: 0.838\n"
     ]
    }
   ],
   "source": [
    "# Performance test - Feature 3\n",
    "test_set = [(gender_features3(n), g) for (n,g) in test]\n",
    "test_set_3 = nltk.classify.accuracy(classifier, test_set)\n",
    "print(\"The accuracy for the test using Feature 3 is: \" + str(test_set_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features4(name):\n",
    "    \n",
    "    features = {}\n",
    "    keywords = [''.join(i) for i in itertools.product(ascii_lowercase, repeat = 2)]\n",
    "    \n",
    "    #look at first, first2, last, last2 letters of name\n",
    "    #apply .lower() method to convert all text to lowercase\n",
    "    features[\"first_letter\"] = name[0].lower()\n",
    "    features[\"first_2letter\"] = name[0:1].lower()\n",
    "    features[\"last_letter\"] = name[-1].lower()\n",
    "    features[\"last_2letter\"] = name[-2:-1].lower()\n",
    "    \n",
    "    for letter in ascii_lowercase:\n",
    "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
    "\n",
    "        for keyword in keywords:\n",
    "            features[\"combo2({})\".format(keyword)] = (keyword in name.lower())\n",
    "            \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the dev using Feature 3 is: 0.8\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features4(n), g) for (n,g) in training]\n",
    "dev_test_set = [(gender_features4(n), g) for (n,g) in dev_test]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "acc_dev_test_4 = nltk.classify.accuracy(classifier, dev_test_set)\n",
    "print(\"The accuracy for the dev using Feature 3 is: \" + str(acc_dev_test_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the test using Feature 4 is: 0.834\n"
     ]
    }
   ],
   "source": [
    "# Performance test - Feature 4\n",
    "test_set = [(gender_features4(n), g) for (n,g) in test]\n",
    "test_set_4 = nltk.classify.accuracy(classifier, test_set)\n",
    "print(\"The accuracy for the test using Feature 4 is: \" + str(test_set_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(gender_features):\n",
    "    errors = []\n",
    "    for (name, tag) in dev_test:\n",
    "        guess = classifier.classify(gender_features(name))\n",
    "        if guess != tag:\n",
    "            errors.append((tag, guess, name))\n",
    "    print('no. of errors: ', len(errors))        \n",
    "        \n",
    "    #for (tag, guess, name) in sorted(errors): \n",
    "    #    print('correct=%-8s guess=%-8s name=%-30s' % (tag, guess, name))\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of errors:  183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('male', 'female', 'Town'),\n",
       " ('male', 'female', 'Ferdy'),\n",
       " ('male', 'female', 'Ephraim'),\n",
       " ('male', 'female', 'Kin'),\n",
       " ('male', 'female', 'Bart'),\n",
       " ('male', 'female', 'Seamus'),\n",
       " ('male', 'female', 'Wilden'),\n",
       " ('male', 'female', 'Noland'),\n",
       " ('male', 'female', 'Bennet'),\n",
       " ('male', 'female', 'Sylvester')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst1 = error_analysis(gender_features1)\n",
    "lst1[0: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of errors:  183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('male', 'female', 'Town'),\n",
       " ('male', 'female', 'Ferdy'),\n",
       " ('male', 'female', 'Ephraim'),\n",
       " ('male', 'female', 'Kin'),\n",
       " ('male', 'female', 'Bart'),\n",
       " ('male', 'female', 'Seamus'),\n",
       " ('male', 'female', 'Wilden'),\n",
       " ('male', 'female', 'Noland'),\n",
       " ('male', 'female', 'Bennet'),\n",
       " ('male', 'female', 'Sylvester')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst2 = error_analysis(gender_features2)\n",
    "lst2[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of errors:  183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('male', 'female', 'Town'),\n",
       " ('male', 'female', 'Ferdy'),\n",
       " ('male', 'female', 'Ephraim'),\n",
       " ('male', 'female', 'Kin'),\n",
       " ('male', 'female', 'Bart'),\n",
       " ('male', 'female', 'Seamus'),\n",
       " ('male', 'female', 'Wilden'),\n",
       " ('male', 'female', 'Noland'),\n",
       " ('male', 'female', 'Bennet'),\n",
       " ('male', 'female', 'Sylvester')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst3 = error_analysis(gender_features3)\n",
    "lst3[0:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of errors:  100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('male', 'female', 'Kin'),\n",
       " ('female', 'male', 'Fay'),\n",
       " ('female', 'male', 'Avril'),\n",
       " ('female', 'male', 'Teryl'),\n",
       " ('male', 'female', 'Noland'),\n",
       " ('male', 'female', 'Bennet'),\n",
       " ('female', 'male', 'Cookie'),\n",
       " ('female', 'male', 'Bamby'),\n",
       " ('male', 'female', 'Carmine'),\n",
       " ('female', 'male', 'Hester')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst4 = error_analysis(gender_features4)\n",
    "lst4[0:10] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Comparition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Dev Feature 1: 0.792\n",
      "Accuracy Test Feature 1: 0.804\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Dev Feature 1: \" + str(acc_dev_test_1))\n",
    "print(\"Accuracy Test Feature 1: \" + str(test_set_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Dev Feature 2: 0.81\n",
      "Accuracy Test Feature 2: 0.822\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Dev Feature 2: \" + str(acc_dev_test_2))\n",
    "print(\"Accuracy Test Feature 2: \" + str(test_set_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Dev Feature 3: 0.826\n",
      "Accuracy Test Feature 3: 0.838\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Dev Feature 3: \" + str(acc_dev_test_3))\n",
    "print(\"Accuracy Test Feature 3: \" + str(test_set_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Dev Feature 4: 0.8\n",
      "Accuracy Test Feature 4: 0.834\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Dev Feature 4: \" + str(acc_dev_test_4))\n",
    "print(\"Accuracy Test Feature 4: \" + str(test_set_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AccuracySimulation(numIterations, callBackFunction):\n",
    "    acc_df = {\n",
    "        \"classifier\": [],\n",
    "        \"test_set_accuracy\": [],\n",
    "        \"dev_test_set_accuracy\": [],\n",
    "        \"train_set_accuracy\": [],\n",
    "        \"dev_test_errors\": []\n",
    "    }\n",
    "    for i in range(numIterations):\n",
    "        random.shuffle(names)\n",
    "        acc_train_names = names[1000:]\n",
    "        acc_dev_test_names = names[500:1000]\n",
    "        acc_test_names = names[:500]\n",
    "        acc_train_set = [(callBackFunction(n), g) for (n,g) in acc_train_names]\n",
    "        acc_dev_test_set = [(callBackFunction(n), g) for (n,g) in acc_dev_test_names]\n",
    "        acc_test_set = [(callBackFunction(n), g) for (n,g) in acc_test_names]\n",
    "        acc_classifier = nltk.NaiveBayesClassifier.train(acc_train_set)\n",
    "        acc_df[\"classifier\"].append(acc_classifier) \n",
    "        acc_df[\"test_set_accuracy\"].append(nltk.classify.accuracy(acc_classifier, acc_test_set))\n",
    "        acc_df[\"dev_test_set_accuracy\"].append(nltk.classify.accuracy(acc_classifier, acc_dev_test_set))\n",
    "        acc_df[\"train_set_accuracy\"].append(nltk.classify.accuracy(acc_classifier, acc_train_set))\n",
    "       \n",
    "        acc_errors = []\n",
    "        for (name, tag) in acc_dev_test_names:\n",
    "            acc_guess = acc_classifier.classify(callBackFunction(name))\n",
    "            if acc_guess != tag:\n",
    "                acc_errors.append( (tag, acc_guess, name) )\n",
    "        acc_df[\"dev_test_errors\"].append(acc_errors)\n",
    "    acc_df = pd.DataFrame.from_dict(acc_df)\n",
    "    return(acc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_set_accuracy</th>\n",
       "      <th>dev_test_set_accuracy</th>\n",
       "      <th>train_set_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.789400</td>\n",
       "      <td>0.790600</td>\n",
       "      <td>0.800648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.019529</td>\n",
       "      <td>0.026867</td>\n",
       "      <td>0.002707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.796947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.781500</td>\n",
       "      <td>0.791000</td>\n",
       "      <td>0.798387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.791000</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>0.800619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.802239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.805732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test_set_accuracy  dev_test_set_accuracy  train_set_accuracy\n",
       "count          10.000000              10.000000           10.000000\n",
       "mean            0.789400               0.790600            0.800648\n",
       "std             0.019529               0.026867            0.002707\n",
       "min             0.746000               0.740000            0.796947\n",
       "25%             0.781500               0.791000            0.798387\n",
       "50%             0.791000               0.795000            0.800619\n",
       "75%             0.803000               0.803000            0.802239\n",
       "max             0.816000               0.826000            0.805732"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = AccuracySimulation(10, gender_features1)\n",
    "df_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_set_accuracy</th>\n",
       "      <th>dev_test_set_accuracy</th>\n",
       "      <th>train_set_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.804400</td>\n",
       "      <td>0.802400</td>\n",
       "      <td>0.831236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.023377</td>\n",
       "      <td>0.016675</td>\n",
       "      <td>0.002461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.758000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.826469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.788000</td>\n",
       "      <td>0.793500</td>\n",
       "      <td>0.830717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.802000</td>\n",
       "      <td>0.831797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.809500</td>\n",
       "      <td>0.832049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.834533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test_set_accuracy  dev_test_set_accuracy  train_set_accuracy\n",
       "count          10.000000              10.000000           10.000000\n",
       "mean            0.804400               0.802400            0.831236\n",
       "std             0.023377               0.016675            0.002461\n",
       "min             0.758000               0.770000            0.826469\n",
       "25%             0.788000               0.793500            0.830717\n",
       "50%             0.811000               0.802000            0.831797\n",
       "75%             0.822000               0.809500            0.832049\n",
       "max             0.832000               0.832000            0.834533"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = AccuracySimulation(10, gender_features2)\n",
    "df_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_set_accuracy</th>\n",
       "      <th>dev_test_set_accuracy</th>\n",
       "      <th>train_set_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.00000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.83020</td>\n",
       "      <td>0.833600</td>\n",
       "      <td>0.862572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.01785</td>\n",
       "      <td>0.019179</td>\n",
       "      <td>0.001615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.79000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.860167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.83200</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.861967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.83300</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.862255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.83400</td>\n",
       "      <td>0.838500</td>\n",
       "      <td>0.862615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.85200</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.865639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test_set_accuracy  dev_test_set_accuracy  train_set_accuracy\n",
       "count           10.00000              10.000000           10.000000\n",
       "mean             0.83020               0.833600            0.862572\n",
       "std              0.01785               0.019179            0.001615\n",
       "min              0.79000               0.800000            0.860167\n",
       "25%              0.83200               0.825000            0.861967\n",
       "50%              0.83300               0.830000            0.862255\n",
       "75%              0.83400               0.838500            0.862615\n",
       "max              0.85200               0.872000            0.865639"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3 = AccuracySimulation(10, gender_features3)\n",
    "df_3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_set_accuracy</th>\n",
       "      <th>dev_test_set_accuracy</th>\n",
       "      <th>train_set_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.806200</td>\n",
       "      <td>0.799800</td>\n",
       "      <td>0.815380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.017473</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.001927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.782000</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>0.812212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.791000</td>\n",
       "      <td>0.795500</td>\n",
       "      <td>0.814264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.815524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.816352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.834000</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.818980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test_set_accuracy  dev_test_set_accuracy  train_set_accuracy\n",
       "count          10.000000              10.000000           10.000000\n",
       "mean            0.806200               0.799800            0.815380\n",
       "std             0.017473               0.015929            0.001927\n",
       "min             0.782000               0.768000            0.812212\n",
       "25%             0.791000               0.795500            0.814264\n",
       "50%             0.804000               0.803000            0.815524\n",
       "75%             0.820000               0.810000            0.816352\n",
       "max             0.834000               0.818000            0.818980"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_4 = AccuracySimulation(10, gender_features4)\n",
    "df_4.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "#### - We found that feature 3 performed better than all the other features.\n",
    "#### - When comparing dev and test sets we found difference but were not significant. This was as expected.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
